{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 4 Euclidean Vector Spaces\n",
        "\n",
        "Let us investigate the inner products and projections in $\\mathbf{R}^n$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lwRCRj-Dqq1L"
      },
      "outputs": [],
      "source": [
        "# numerical and scientific computing libraries  \n",
        "import numpy as np \n",
        "import scipy as sp\n",
        "\n",
        "# plotting libraries\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for pretty printing\n",
        "np.set_printoptions(4, linewidth=100, suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standard inner product or dot product in $\\mathbb{R}^n$.\n",
        "\n",
        "For two Euclidean vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$, we define the standard inner product or dot product as\n",
        "$$<\\mathbf{x}, \\mathbf{y}> = \\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^n x_i y_i$$\n",
        "Even though there are infinitely many inner products, as we have seen in the textbook, the standard inner product is a special one, on which our most numerics on the length and angle on the earth are based. Probably you might already encountered this concept (partially at least)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting dimension\n",
        "n = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a =  [ 0.0672  0.048  -0.029   1.1152  0.5597 -0.074  -0.2288 -0.3743  1.35    0.2169]\n",
            "b =  [ 0.367   0.6792  0.7419 -1.0125 -0.5873  0.7989  0.9749 -2.6531 -1.3734 -0.9072]\n",
            "-2.7620943477595925 -2.7620943477595925 -2.7620943477595925 -2.762094347759592\n"
          ]
        }
      ],
      "source": [
        "# two vectors generated randomly\n",
        "a = np.random.randn(n)\n",
        "b = np.random.randn(n)\n",
        "print('a = ',a)\n",
        "print('b = ',b)\n",
        "# many ways of computing the dot product\n",
        "ip1 = np.inner(a,b)\n",
        "ip2 = np.dot(a,b)\n",
        "ip3 = np.sum(a[:]*b[:])\n",
        "ip4 = sum(a[:]*b[:])\n",
        "# In principle, all these produce a single value. Be careful in using np.inner and np.dot when a and b are multi-dimensional.\n",
        "print(ip1, ip2, ip3, ip4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a given inner product $<\\cdot,\\cdot>$, the associated norm of a vector $\\mathbf{v}$ is defined as $|\\mathbf{v}| = \\sqrt{<\\mathbf{v}, \\mathbf{v}>}$. With the standard inner product, its norm or length is $|\\mathbf{x}| = \\sqrt{\\mathbf{x}^\\top \\mathbf{x}}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|a| =  1.905818636783195 1.905818636783195\n",
            "|c| =  1.0\n"
          ]
        }
      ],
      "source": [
        "# Define a norm for standard inner product\n",
        "def norm_scratch(v):\n",
        "    return np.sqrt(sum(v**2))\n",
        "\n",
        "print('|a| = ', norm_scratch(a), np.linalg.norm(a))\n",
        "# normalize a vector\n",
        "c = (1/norm_scratch(a))*a\n",
        "print('|c| = ', norm_scratch(c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**One-dimensional projection**\n",
        "\n",
        "Now consider a one-dimensional projection along a vector $\\mathbf{v}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9999999999999999\n",
            "angle =  106.87119655132207\n",
            "a      =  [ 0.0672  0.048  -0.029   1.1152  0.5597 -0.074  -0.2288 -0.3743  1.35    0.2169]\n",
            "v      =  [-0.4304 -0.4212  0.0719  0.0622 -0.593   0.2492  0.2723 -0.3008 -0.1795 -0.1328]\n",
            "c1 =  -0.5531088808981259\n",
            "a_proj =  [ 0.2381  0.233  -0.0398 -0.0344  0.328  -0.1378 -0.1506  0.1664  0.0993  0.0735]\n"
          ]
        }
      ],
      "source": [
        "v1 = np.random.randn(n)\n",
        "norm_v1 = norm_scratch(v1)\n",
        "v = (1/norm_v1)*v1\n",
        "\n",
        "#check the unity of v\n",
        "print(norm_scratch(v))\n",
        "\n",
        "# project vector a along the direction of v\n",
        "c1 = np.inner(a,v)\n",
        "a_proj = c1*v\n",
        "cosine = c1/norm_scratch(a)\n",
        "print('angle = ', np.degrees(np.arccos(cosine)))\n",
        "\n",
        "print('a      = ', a)\n",
        "print('v      = ', v)\n",
        "print('c1 = ', c1)\n",
        "print('a_proj = ',a_proj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Gram-Schmidt procedure**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting dimension\n",
        "n = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "AVNILtkTH_sp",
        "outputId": "4a76b9e8-7463-48d8-d638-32cc92230688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.0399 -0.4094 -0.0293]\n",
            " [ 0.2972 -0.0212 -0.3281]\n",
            " [-0.012   0.1363  0.2737]\n",
            " [ 0.134   0.337  -0.2834]\n",
            " [ 0.4604  0.1595  0.6304]\n",
            " [ 0.4559 -0.1455  0.0984]\n",
            " [ 0.3304  0.5502 -0.3612]\n",
            " [-0.068  -0.3564 -0.068 ]\n",
            " [ 0.4396 -0.2202  0.1824]\n",
            " [ 0.4062 -0.4182 -0.401 ]]\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n",
            "[[ 0.1701  0.0065 -0.0633 -0.135  -0.1021  0.0385 -0.2279  0.1506  0.0673  0.1668]\n",
            " [ 0.0065  0.1965 -0.0963  0.1257 -0.0734  0.1063  0.2051  0.0096  0.0755  0.2612]\n",
            " [-0.0633 -0.0963  0.0936 -0.0333  0.1887  0.0016 -0.0278 -0.0663  0.0146 -0.1716]\n",
            " [-0.135   0.1257 -0.0333  0.2119 -0.0632 -0.0158  0.3321 -0.11   -0.067   0.0272]\n",
            " [-0.1021 -0.0734  0.1887 -0.0632  0.6348  0.2487  0.0122 -0.131   0.2823 -0.1325]\n",
            " [ 0.0385  0.1063  0.0016 -0.0158  0.2487  0.2387  0.0351  0.0141  0.2504  0.2066]\n",
            " [-0.2279  0.2051 -0.0278  0.3321  0.0122  0.0351  0.5424 -0.194  -0.0418  0.0489]\n",
            " [ 0.1506  0.0096 -0.0663 -0.11   -0.131   0.0141 -0.194   0.1363  0.0362  0.1487]\n",
            " [ 0.0673  0.0755  0.0146 -0.067   0.2823  0.2504 -0.0418  0.0362  0.2751  0.1975]\n",
            " [ 0.1668  0.2612 -0.1716  0.0272 -0.1325  0.2066  0.0489  0.1487  0.1975  0.5007]]\n"
          ]
        }
      ],
      "source": [
        "a1 = np.random.randn(n)\n",
        "a2 = np.random.randn(n)\n",
        "a3 = np.random.randn(n)\n",
        "\n",
        "# the first orthonormal vector\n",
        "v1 = (1/norm_scratch(a1))*a1\n",
        "\n",
        "# the second orthonormal vector\n",
        "v = a2 - np.inner(a2,v1)*v1\n",
        "v2 = (1/norm_scratch(v))*v\n",
        "\n",
        "# the third orthonormal vector\n",
        "v = a3 - np.inner(a3,v1)*v1 - np.inner(a3,v2)*v2\n",
        "v3 = (1/norm_scratch(v))*v\n",
        "\n",
        "# build a matrix of orthonormal vectors\n",
        "Q = np.stack((v1,v2,v3),axis=1)\n",
        "\n",
        "print(Q)\n",
        "print(Q.T @ Q)  # As you expect, it is an identity matrix of 3x3\n",
        "print(Q @ Q.T)  # It is meaningless matrix of 10x10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example for 3D visualization (if n=3)\n",
        "if n==3:\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Plot original vectors\n",
        "    ax.quiver(0, 0, 0, a1[0], a1[1], a1[2], color='r', label='a1')\n",
        "    ax.quiver(0, 0, 0, a2[0], a2[1], a2[2], color='g', label='a2')\n",
        "    ax.quiver(0, 0, 0, a3[0], a3[1], a3[2], color='b', label='a3')\n",
        "\n",
        "    # Plot orthonormal vectors\n",
        "    ax.quiver(0, 0, 0, v1[0], v1[1], v1[2], color='c', label='v1 (orthonormal)')\n",
        "    ax.quiver(0, 0, 0, v2[0], v2[1], v2[2], color='m', label='v2 (orthonormal)')\n",
        "    ax.quiver(0, 0, 0, v3[0], v3[1], v3[2], color='y', label='v3 (orthonormal)')\n",
        "\n",
        "    ax.set_xlim([-2, 2])\n",
        "    ax.set_ylim([-2, 2])\n",
        "    ax.set_zlim([-2, 2])\n",
        "    ax.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For $n > 3$, let us go further to find a orthonormal basis of $\\mathbb{R}^n$. In the textbook, three vectors can not span $\\mathbb{R}^n$ and there exist a vector not in the subspace spanned by the three vectors. However, nobody provide us the desired vector ourside of the subspace. We have to search for one now. One idea would be considering the standard basic vector $\\mathbf{e}_i$ one by one. Here, along a popular idea in data science, we borrow probabilistic thinking: the subspace of dimension less than $n$ has a zero-volume in $\\mathbb{R}^n$ and hence if we sample a vector randomly in $\\mathbb{R}^n$, then it would be outside of the subspace with very high probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "k = 3\n",
        "Q = np.stack((v1,v2,v3),axis=1)\n",
        "\n",
        "while k < n:\n",
        "    a = np.random.randn(n)\n",
        "    for i in range(k):\n",
        "        a = a - np.inner(a,Q[:,i])*Q[:,i]\n",
        "    if norm_scratch(a) > 1e-10:\n",
        "        Q = np.concatenate((Q, (1/norm_scratch(a))*a[:,np.newaxis]), axis=1)\n",
        "        k = k + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Surprisingly, the lines in the above cell is written down 100% by co-pilot!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.0399 -0.4094 -0.0293  0.2372  0.6643 -0.0705  0.3733 -0.0405  0.4224  0.0892]\n",
            " [ 0.2972 -0.0212 -0.3281  0.5763 -0.1809  0.6279 -0.097  -0.1192  0.1034  0.1006]\n",
            " [-0.012   0.1363  0.2737  0.394   0.0393 -0.3888 -0.4753 -0.036   0.0914  0.6024]\n",
            " [ 0.134   0.337  -0.2834  0.0692 -0.2465 -0.4353  0.2834 -0.5845  0.3133 -0.1138]\n",
            " [ 0.4604  0.1595  0.6304 -0.232   0.1222  0.2695 -0.1321 -0.2291  0.3549 -0.1674]\n",
            " [ 0.4559 -0.1455  0.0984  0.199  -0.3564 -0.3154  0.2123  0.6227  0.2066 -0.1398]\n",
            " [ 0.3304  0.5502 -0.3612 -0.2925  0.3628  0.0643  0.1013  0.3186  0.0103  0.3527]\n",
            " [-0.068  -0.3564 -0.068  -0.4669 -0.4063  0.1525  0.1016 -0.0552  0.356   0.5633]\n",
            " [ 0.4396 -0.2202  0.1824  0.0207  0.0203 -0.0706  0.379  -0.2841 -0.6389  0.2942]\n",
            " [ 0.4062 -0.4182 -0.401  -0.2328  0.1565 -0.2359 -0.5642 -0.1252 -0.0264 -0.1739]]\n",
            "[[ 1.  0.  0.  0. -0.  0.  0. -0. -0.  0.]\n",
            " [ 0.  1.  0.  0.  0. -0.  0.  0. -0. -0.]\n",
            " [ 0.  0.  1.  0. -0.  0.  0. -0. -0. -0.]\n",
            " [ 0.  0.  0.  1. -0.  0.  0. -0.  0. -0.]\n",
            " [-0.  0. -0. -0.  1. -0. -0.  0. -0.  0.]\n",
            " [ 0. -0.  0.  0. -0.  1.  0. -0. -0.  0.]\n",
            " [ 0.  0.  0.  0. -0.  0.  1. -0.  0.  0.]\n",
            " [-0.  0. -0. -0.  0. -0. -0.  1. -0. -0.]\n",
            " [-0. -0. -0.  0. -0. -0.  0. -0.  1.  0.]\n",
            " [ 0. -0. -0. -0.  0.  0.  0. -0.  0.  1.]]\n",
            "[[ 1.  0.  0.  0.  0. -0. -0.  0. -0. -0.]\n",
            " [ 0.  1.  0.  0.  0. -0.  0.  0.  0. -0.]\n",
            " [ 0.  0.  1.  0.  0. -0. -0. -0.  0.  0.]\n",
            " [ 0.  0.  0.  1.  0. -0. -0.  0. -0. -0.]\n",
            " [ 0.  0.  0.  0.  1. -0. -0.  0.  0. -0.]\n",
            " [-0. -0. -0. -0. -0.  1.  0. -0.  0. -0.]\n",
            " [-0.  0. -0. -0. -0.  0.  1. -0.  0.  0.]\n",
            " [ 0.  0. -0.  0.  0. -0. -0.  1.  0.  0.]\n",
            " [-0.  0.  0. -0.  0.  0.  0.  0.  1. -0.]\n",
            " [-0. -0.  0. -0. -0. -0.  0.  0. -0.  1.]]\n"
          ]
        }
      ],
      "source": [
        "print(Q)\n",
        "print(Q.T @ Q)  # As you expect, it is an identity matrix of 10x10\n",
        "print(Q @ Q.T)  # Now it is an identity matrix of 10x10"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "linear_algebra",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
